{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029fa83a-5941-4629-b43c-0138c40febc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers boto3 sagemaker mlflow dagshub s3fs -q"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T09:39:47.294442Z",
     "start_time": "2024-06-08T09:39:47.284270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "1afb7f3ef0af498b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a58c924-c3f3-45c9-bcbf-05c7ddc302d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "role = os.getenv(\"AWS_ADMIN_ROLE\")"
  },
  {
   "cell_type": "markdown",
   "id": "d4db4c4a-0477-4c3b-a185-ed9233eee9d1",
   "metadata": {},
   "source": [
    "## Set mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11c2cc91-1fce-4d7d-87a9-8fb7fb7f43c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_server_uri = os.getenv(\"REMOTE_SERVER_URI\")\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806de86-9741-4d99-a8af-6660e48b037c",
   "metadata": {},
   "source": [
    "## Get sentences and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bb3a7af-cd55-4a60-9df0-27e23acabbed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = os.getenv(\"S3_BUCKER_NAME\")\n",
    "dataset = 'imdb_dataset.csv'\n",
    "data = pd.read_csv(f\"s3://{bucket}/{dataset}\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d5a475e-d10f-477a-93e5-891fe664c169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = dict({'positive': 1, 'negative': 0})\n",
    "data['sentiment'] = data['sentiment'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf45c7d0-b7c5-474c-9ecb-da006e0427a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data)\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "test.to_csv(\"./data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52768b91-b845-4c9f-848d-60cdd95ab900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Karen Carpenter Story shows a little more ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I had the terrible misfortune of having to vie...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "5   Probably my all-time favorite movie, a story o...  positive\n",
       "10  Phil the Alien is one of those quirky films wh...  negative\n",
       "25  The Karen Carpenter Story shows a little more ...  positive\n",
       "2   I thought this was a wonderful way to spend ti...  positive\n",
       "21  I had the terrible misfortune of having to vie...  negative"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72216328-7943-4995-b760-8cacc2c90822",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f12fac52-c339-4171-a626-530c1e5a836c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"./data/train.csv\", bucket=bucket, key_prefix=prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"./data/test.csv\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974c973-44aa-4a54-baff-35076b91399a",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44f735-36bd-4780-a2c7-be31df4210ea",
   "metadata": {},
   "source": [
    "We use the PyTorch-Transformers library, which contains PyTorch implementations and pre-trained model weights for many NLP models, including BERT.\n",
    "\n",
    "Our training script should save model artifacts learned during training to a file path called model_dir, as stipulated by the SageMaker PyTorch image. Upon completion of training, model artifacts saved in model_dir will be uploaded to S3 by SageMaker and will become available in S3 for deployment.\n",
    "\n",
    "We save this script in a file named train_deploy.py, and put the file in a directory named code/. The full training script can be viewed under code/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99acfb22-2061-4393-890f-e1efcbd6e788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36margparse\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mjson\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mlogging\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mos\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36msys\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mnumpy\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mnp\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mpandas\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mpd\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdistributed\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mdist\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mutils\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdata\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mutils\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdata\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdistributed\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtorch\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mutils\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mdata\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m DataLoader, RandomSampler, TensorDataset\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36mtransformers\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m AdamW, BertForSequenceClassification, BertTokenizer\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mmlflow\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mfrom\u001B[39;49;00m \u001B[04m\u001B[36msklearn\u001B[39;49;00m\u001B[04m\u001B[36m.\u001B[39;49;00m\u001B[04m\u001B[36mmetrics\u001B[39;49;00m \u001B[34mimport\u001B[39;49;00m f1_score\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "logger = logging.getLogger(\u001B[31m__name__\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "logger.setLevel(logging.DEBUG)\u001B[37m\u001B[39;49;00m\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "MAX_LEN = \u001B[34m1024\u001B[39;49;00m  \u001B[37m# this is the max length of the sentence\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mLoading BERT tokenizer...\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "tokenizer = BertTokenizer.from_pretrained(\u001B[33m\"\u001B[39;49;00m\u001B[33mbert-base-uncased\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, do_lower_case=\u001B[34mTrue\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mflat_accuracy\u001B[39;49;00m(preds, labels):\u001B[37m\u001B[39;49;00m\n",
      "    pred_flat = np.argmax(preds, axis=\u001B[34m1\u001B[39;49;00m).flatten()\u001B[37m\u001B[39;49;00m\n",
      "    labels_flat = labels.flatten()\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mreturn\u001B[39;49;00m np.sum(pred_flat == labels_flat) / \u001B[36mlen\u001B[39;49;00m(labels_flat)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_get_train_data_loader\u001B[39;49;00m(batch_size, training_dir, is_distributed):\u001B[37m\u001B[39;49;00m\n",
      "    logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mGet train data loader\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001B[33m\"\u001B[39;49;00m\u001B[33mtrain.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[37m\u001B[39;49;00m\n",
      "    sentences = dataset.review.values\u001B[37m\u001B[39;49;00m\n",
      "    labels = dataset.sentiment.values\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    input_ids = []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mfor\u001B[39;49;00m sent \u001B[35min\u001B[39;49;00m sentences:\u001B[37m\u001B[39;49;00m\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001B[34mTrue\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        input_ids.append(encoded_sent)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# pad shorter sentences\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    input_ids_padded = []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mfor\u001B[39;49;00m i \u001B[35min\u001B[39;49;00m input_ids:\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mwhile\u001B[39;49;00m \u001B[36mlen\u001B[39;49;00m(i) < MAX_LEN:\u001B[37m\u001B[39;49;00m\n",
      "            i.append(\u001B[34m0\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        input_ids_padded.append(i)\u001B[37m\u001B[39;49;00m\n",
      "    input_ids = input_ids_padded\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# mask; 0: added, 1: otherwise\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    attention_masks = []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# For each sentence...\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mfor\u001B[39;49;00m sent \u001B[35min\u001B[39;49;00m input_ids:\u001B[37m\u001B[39;49;00m\n",
      "        att_mask = [\u001B[36mint\u001B[39;49;00m(token_id > \u001B[34m0\u001B[39;49;00m) \u001B[34mfor\u001B[39;49;00m token_id \u001B[35min\u001B[39;49;00m sent]\u001B[37m\u001B[39;49;00m\n",
      "        attention_masks.append(att_mask)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    input_ids = np.array(input_ids, dtype=np.int64)\u001B[37m\u001B[39;49;00m\n",
      "    labels = np.array(labels, dtype=np.int64)\u001B[37m\u001B[39;49;00m\n",
      "    attention_masks = np.array(attention_masks, dtype=np.int64)  \u001B[37m# Or dtype=np.float32 if they're floats\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# convert to PyTorch data types.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    train_inputs = torch.tensor(input_ids)\u001B[37m\u001B[39;49;00m\n",
      "    train_labels = torch.tensor(labels)\u001B[37m\u001B[39;49;00m\n",
      "    train_masks = torch.tensor(attention_masks)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mif\u001B[39;49;00m is_distributed:\u001B[37m\u001B[39;49;00m\n",
      "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34melse\u001B[39;49;00m:\u001B[37m\u001B[39;49;00m\n",
      "        train_sampler = RandomSampler(train_data)\u001B[37m\u001B[39;49;00m\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mreturn\u001B[39;49;00m train_dataloader\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_get_test_data_loader\u001B[39;49;00m(test_batch_size, training_dir):\u001B[37m\u001B[39;49;00m\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001B[33m\"\u001B[39;49;00m\u001B[33mtest.csv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m))\u001B[37m\u001B[39;49;00m\n",
      "    sentences = dataset.review.values\u001B[37m\u001B[39;49;00m\n",
      "    labels = dataset.sentiment.values\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    input_ids = []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mfor\u001B[39;49;00m sent \u001B[35min\u001B[39;49;00m sentences:\u001B[37m\u001B[39;49;00m\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001B[34mTrue\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        input_ids.append(encoded_sent)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# pad shorter sentences\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    input_ids_padded = []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mfor\u001B[39;49;00m i \u001B[35min\u001B[39;49;00m input_ids:\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mwhile\u001B[39;49;00m \u001B[36mlen\u001B[39;49;00m(i) < MAX_LEN:\u001B[37m\u001B[39;49;00m\n",
      "            i.append(\u001B[34m0\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        input_ids_padded.append(i)\u001B[37m\u001B[39;49;00m\n",
      "    input_ids = input_ids_padded\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# mask; 0: added, 1: otherwise\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    attention_masks = []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# For each sentence...\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mfor\u001B[39;49;00m sent \u001B[35min\u001B[39;49;00m input_ids:\u001B[37m\u001B[39;49;00m\n",
      "        att_mask = [\u001B[36mint\u001B[39;49;00m(token_id > \u001B[34m0\u001B[39;49;00m) \u001B[34mfor\u001B[39;49;00m token_id \u001B[35min\u001B[39;49;00m sent]\u001B[37m\u001B[39;49;00m\n",
      "        attention_masks.append(att_mask)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m\u001B[39;49;00m\n",
      "    input_ids = np.array(input_ids, dtype=np.int64)\u001B[37m\u001B[39;49;00m\n",
      "    labels = np.array(labels, dtype=np.int64)\u001B[37m\u001B[39;49;00m\n",
      "    attention_masks = np.array(attention_masks, dtype=np.int64)  \u001B[37m# Or dtype=np.float32 if they're floats\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# convert to PyTorch data types.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    train_inputs = torch.tensor(input_ids)\u001B[37m\u001B[39;49;00m\n",
      "    train_labels = torch.tensor(labels)\u001B[37m\u001B[39;49;00m\n",
      "    train_masks = torch.tensor(attention_masks)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\u001B[37m\u001B[39;49;00m\n",
      "    train_sampler = RandomSampler(train_data)\u001B[37m\u001B[39;49;00m\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mreturn\u001B[39;49;00m train_dataloader\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mtrain\u001B[39;49;00m(args):\u001B[37m\u001B[39;49;00m\n",
      "    is_distributed = \u001B[36mlen\u001B[39;49;00m(args.hosts) > \u001B[34m1\u001B[39;49;00m \u001B[35mand\u001B[39;49;00m args.backend \u001B[35mis\u001B[39;49;00m \u001B[35mnot\u001B[39;49;00m \u001B[34mNone\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    logger.debug(\u001B[33m\"\u001B[39;49;00m\u001B[33mDistributed training - \u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, is_distributed)\u001B[37m\u001B[39;49;00m\n",
      "    use_cuda = args.num_gpus > \u001B[34m0\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    logger.debug(\u001B[33m\"\u001B[39;49;00m\u001B[33mNumber of gpus available - \u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, args.num_gpus)\u001B[37m\u001B[39;49;00m\n",
      "    device = torch.device(\u001B[33m\"\u001B[39;49;00m\u001B[33mcuda\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[34mif\u001B[39;49;00m use_cuda \u001B[34melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mcpu\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mif\u001B[39;49;00m is_distributed:\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# Initialize the distributed environment.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        world_size = \u001B[36mlen\u001B[39;49;00m(args.hosts)\u001B[37m\u001B[39;49;00m\n",
      "        os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mWORLD_SIZE\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m] = \u001B[36mstr\u001B[39;49;00m(world_size)\u001B[37m\u001B[39;49;00m\n",
      "        host_rank = args.hosts.index(args.current_host)\u001B[37m\u001B[39;49;00m\n",
      "        os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mRANK\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m] = \u001B[36mstr\u001B[39;49;00m(host_rank)\u001B[37m\u001B[39;49;00m\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\u001B[37m\u001B[39;49;00m\n",
      "        logger.info(\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mInitialized the distributed environment: \u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33m backend on \u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m nodes. \u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mCurrent host rank is \u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m. Number of gpus: \u001B[39;49;00m\u001B[33m%d\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "            args.backend, dist.get_world_size(),\u001B[37m\u001B[39;49;00m\n",
      "            dist.get_rank(), args.num_gpus\u001B[37m\u001B[39;49;00m\n",
      "        )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# set the seed for generating random numbers\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    torch.manual_seed(args.seed)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mif\u001B[39;49;00m use_cuda:\u001B[37m\u001B[39;49;00m\n",
      "        torch.cuda.manual_seed(args.seed)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed)\u001B[37m\u001B[39;49;00m\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    remote_server_uri = \u001B[33m\"\u001B[39;49;00m\u001B[33mhttp://ec2-13-60-68-88.eu-north-1.compute.amazonaws.com:5000/\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.set_tracking_uri(remote_server_uri)\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.pytorch.autolog()\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.set_experiment(\u001B[33m\"\u001B[39;49;00m\u001B[33mTraining BERT with Sagemaker\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.set_tag(\u001B[33m\"\u001B[39;49;00m\u001B[33mTraining Info\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mTraining BERT on IMDB Dataset\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.log_param(\u001B[33m\"\u001B[39;49;00m\u001B[33mmodel_name\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33mbert-base-uncased\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.log_param(\u001B[33m\"\u001B[39;49;00m\u001B[33mnum_train_epochs\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, args.epochs)\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.log_param(\u001B[33m\"\u001B[39;49;00m\u001B[33mtrain_batch_size\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, args.batch_size)\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.log_param(\u001B[33m\"\u001B[39;49;00m\u001B[33mtest_batch_size\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, args.batch_size)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    logger.debug(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33mProcesses \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m (\u001B[39;49;00m\u001B[33m{:.0f}\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33m) of train data\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[36mlen\u001B[39;49;00m(train_loader.sampler),\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[36mlen\u001B[39;49;00m(train_loader.dataset),\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[34m100.0\u001B[39;49;00m * \u001B[36mlen\u001B[39;49;00m(train_loader.sampler) / \u001B[36mlen\u001B[39;49;00m(train_loader.dataset),\u001B[37m\u001B[39;49;00m\n",
      "        )\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    logger.debug(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33mProcesses \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m (\u001B[39;49;00m\u001B[33m{:.0f}\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33m) of test data\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[36mlen\u001B[39;49;00m(test_loader.sampler),\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[36mlen\u001B[39;49;00m(test_loader.dataset),\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[34m100.0\u001B[39;49;00m * \u001B[36mlen\u001B[39;49;00m(test_loader.sampler) / \u001B[36mlen\u001B[39;49;00m(test_loader.dataset),\u001B[37m\u001B[39;49;00m\n",
      "        )\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mStarting BertForSequenceClassification\u001B[39;49;00m\u001B[33m\\n\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    model = BertForSequenceClassification.from_pretrained(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33mbert-base-uncased\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,  \u001B[37m# Use the 12-layer BERT model, with an uncased vocab.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        num_labels=args.num_labels,  \u001B[37m# The number of output labels--2 for binary classification.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        output_attentions=\u001B[34mFalse\u001B[39;49;00m,  \u001B[37m# Whether the model returns attentions weights.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        output_hidden_states=\u001B[34mFalse\u001B[39;49;00m,  \u001B[37m# Whether the model returns all hidden-states.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    model = model.to(device)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mif\u001B[39;49;00m is_distributed \u001B[35mand\u001B[39;49;00m use_cuda:\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# multi-machine multi-gpu case\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34melse\u001B[39;49;00m:\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        model = torch.nn.DataParallel(model)\u001B[37m\u001B[39;49;00m\n",
      "    optimizer = AdamW(\u001B[37m\u001B[39;49;00m\n",
      "        model.parameters(),\u001B[37m\u001B[39;49;00m\n",
      "        lr=\u001B[34m2e-5\u001B[39;49;00m,  \u001B[37m# args.learning_rate - default is 5e-5, our notebook had 2e-5\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        eps=\u001B[34m1e-8\u001B[39;49;00m,  \u001B[37m# args.adam_epsilon - default is 1e-8.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mwith\u001B[39;49;00m mlflow.start_run():\u001B[37m\u001B[39;49;00m\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mEnd of defining BertForSequenceClassification\u001B[39;49;00m\u001B[33m\\n\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mfor\u001B[39;49;00m epoch \u001B[35min\u001B[39;49;00m \u001B[36mrange\u001B[39;49;00m(\u001B[34m1\u001B[39;49;00m, args.epochs + \u001B[34m1\u001B[39;49;00m):\u001B[37m\u001B[39;49;00m\n",
      "            total_loss = \u001B[34m0\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "            model.train()\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[34mfor\u001B[39;49;00m step, batch \u001B[35min\u001B[39;49;00m \u001B[36menumerate\u001B[39;49;00m(train_loader):\u001B[37m\u001B[39;49;00m\n",
      "                b_input_ids = batch[\u001B[34m0\u001B[39;49;00m].to(device)\u001B[37m\u001B[39;49;00m\n",
      "                b_input_mask = batch[\u001B[34m1\u001B[39;49;00m].to(device)\u001B[37m\u001B[39;49;00m\n",
      "                b_labels = batch[\u001B[34m2\u001B[39;49;00m].to(device)\u001B[37m\u001B[39;49;00m\n",
      "                model.zero_grad()\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "                outputs = model(b_input_ids, token_type_ids=\u001B[34mNone\u001B[39;49;00m, attention_mask=b_input_mask, labels=b_labels)\u001B[37m\u001B[39;49;00m\n",
      "                loss = outputs[\u001B[34m0\u001B[39;49;00m]\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "                total_loss += loss.item()\u001B[37m\u001B[39;49;00m\n",
      "                loss.backward()\u001B[37m\u001B[39;49;00m\n",
      "                torch.nn.utils.clip_grad_norm_(model.parameters(), \u001B[34m1.0\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "                \u001B[37m# modified based on their gradients, the learning rate, etc.\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "                optimizer.step()\u001B[37m\u001B[39;49;00m\n",
      "                \u001B[34mif\u001B[39;49;00m step % args.log_interval == \u001B[34m0\u001B[39;49;00m:\u001B[37m\u001B[39;49;00m\n",
      "                    logger.info(\u001B[37m\u001B[39;49;00m\n",
      "                        \u001B[33m\"\u001B[39;49;00m\u001B[33mTrain Epoch: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m [\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m/\u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m (\u001B[39;49;00m\u001B[33m{:.0f}\u001B[39;49;00m\u001B[33m%\u001B[39;49;00m\u001B[33m)] Loss: \u001B[39;49;00m\u001B[33m{:.6f}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(\u001B[37m\u001B[39;49;00m\n",
      "                            epoch,\u001B[37m\u001B[39;49;00m\n",
      "                            step * \u001B[36mlen\u001B[39;49;00m(batch[\u001B[34m0\u001B[39;49;00m]),\u001B[37m\u001B[39;49;00m\n",
      "                            \u001B[36mlen\u001B[39;49;00m(train_loader.sampler),\u001B[37m\u001B[39;49;00m\n",
      "                            \u001B[34m100.0\u001B[39;49;00m * step / \u001B[36mlen\u001B[39;49;00m(train_loader),\u001B[37m\u001B[39;49;00m\n",
      "                            loss.item(),\u001B[37m\u001B[39;49;00m\n",
      "                        )\u001B[37m\u001B[39;49;00m\n",
      "                    )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "            average_train_loss = total_loss / \u001B[36mlen\u001B[39;49;00m(train_loader)\u001B[37m\u001B[39;49;00m\n",
      "            mlflow.log_metric(\u001B[33m\"\u001B[39;49;00m\u001B[33maverage_loss\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, average_train_loss, step=epoch)\u001B[37m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mAverage training loss: \u001B[39;49;00m\u001B[33m%f\u001B[39;49;00m\u001B[33m\\n\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, average_train_loss)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "            test(model, test_loader, device, epoch)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mSaving tuned model.\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        mlflow.pytorch.log_model(model)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m\u001B[39;49;00m\n",
      "    mlflow.end_run()\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    model_2_save = model.module \u001B[34mif\u001B[39;49;00m \u001B[36mhasattr\u001B[39;49;00m(model, \u001B[33m\"\u001B[39;49;00m\u001B[33mmodule\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m) \u001B[34melse\u001B[39;49;00m model\u001B[37m\u001B[39;49;00m\n",
      "    model_2_save.save_pretrained(save_directory=args.model_dir)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mf1_score_func\u001B[39;49;00m(preds, labels):\u001B[37m\u001B[39;49;00m\n",
      "    preds_flat = np.argmax(preds, axis=\u001B[34m1\u001B[39;49;00m).flatten()\u001B[37m\u001B[39;49;00m\n",
      "    labels_flat = labels.flatten()\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mreturn\u001B[39;49;00m f1_score(labels_flat, preds_flat, average = \u001B[33m'\u001B[39;49;00m\u001B[33mweighted\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mtest\u001B[39;49;00m(model, test_loader, device, epoch):\u001B[37m\u001B[39;49;00m\n",
      "    model.eval()\u001B[37m\u001B[39;49;00m\n",
      "    _, eval_accuracy = \u001B[34m0\u001B[39;49;00m, \u001B[34m0\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    predictions, true_vals = [], []\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mwith\u001B[39;49;00m torch.no_grad():\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mfor\u001B[39;49;00m batch \u001B[35min\u001B[39;49;00m test_loader:\u001B[37m\u001B[39;49;00m\n",
      "            b_input_ids = batch[\u001B[34m0\u001B[39;49;00m].to(device)\u001B[37m\u001B[39;49;00m\n",
      "            b_input_mask = batch[\u001B[34m1\u001B[39;49;00m].to(device)\u001B[37m\u001B[39;49;00m\n",
      "            b_labels = batch[\u001B[34m2\u001B[39;49;00m].to(device)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001B[34mNone\u001B[39;49;00m, attention_mask=b_input_mask)\u001B[37m\u001B[39;49;00m\n",
      "            logits = outputs[\u001B[34m0\u001B[39;49;00m]\u001B[37m\u001B[39;49;00m\n",
      "            logits = logits.detach().cpu().numpy()\u001B[37m\u001B[39;49;00m\n",
      "            label_ids = b_labels.to(\u001B[33m\"\u001B[39;49;00m\u001B[33mcpu\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m).numpy()\u001B[37m\u001B[39;49;00m\n",
      "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\u001B[37m\u001B[39;49;00m\n",
      "            eval_accuracy += tmp_eval_accuracy\u001B[37m\u001B[39;49;00m\n",
      "            predictions.append(logits)\u001B[37m\u001B[39;49;00m\n",
      "            true_vals.append(label_ids)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    val_f1 = f1_score_func(predictions, true_vals)\u001B[37m\u001B[39;49;00m\n",
      "    accuracy = accuracy_per_class(predictions, true_vals)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    mlflow.log_metrics({\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33mval_loss\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: val_loss,\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33mval_f1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: val_f1,\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33maccuracy\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: eval_accuracy,\u001B[37m\u001B[39;49;00m\n",
      "    },  step=epoch)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mTest set: Accuracy: \u001B[39;49;00m\u001B[33m%f\u001B[39;49;00m\u001B[33m\\n\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, tmp_eval_accuracy)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mmodel_fn\u001B[39;49;00m(model_dir):\u001B[37m\u001B[39;49;00m\n",
      "    device = torch.device(\u001B[33m\"\u001B[39;49;00m\u001B[33mcuda\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[34mif\u001B[39;49;00m torch.cuda.is_available() \u001B[34melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mcpu\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m================ objects in model_dir ===================\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[36mprint\u001B[39;49;00m(os.listdir(model_dir))\u001B[37m\u001B[39;49;00m\n",
      "    model = BertForSequenceClassification.from_pretrained(model_dir)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m================ model loaded ===========================\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mreturn\u001B[39;49;00m model.to(device)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32minput_fn\u001B[39;49;00m(request_body, request_content_type):\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m    \u001B[39;49;00m\u001B[33m\"\"\"An input_fn that loads a pickled tensor\"\"\"\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mif\u001B[39;49;00m request_content_type == \u001B[33m\"\u001B[39;49;00m\u001B[33mapplication/json\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m:\u001B[37m\u001B[39;49;00m\n",
      "        data = json.loads(request_body)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m================ input sentences ===============\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(data)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mif\u001B[39;49;00m \u001B[36misinstance\u001B[39;49;00m(data, \u001B[36mstr\u001B[39;49;00m):\u001B[37m\u001B[39;49;00m\n",
      "            data = [data]\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34melif\u001B[39;49;00m \u001B[36misinstance\u001B[39;49;00m(data, \u001B[36mlist\u001B[39;49;00m) \u001B[35mand\u001B[39;49;00m \u001B[36mlen\u001B[39;49;00m(data) > \u001B[34m0\u001B[39;49;00m \u001B[35mand\u001B[39;49;00m \u001B[36misinstance\u001B[39;49;00m(data[\u001B[34m0\u001B[39;49;00m], \u001B[36mstr\u001B[39;49;00m):\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[34mpass\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34melse\u001B[39;49;00m:\u001B[37m\u001B[39;49;00m\n",
      "            \u001B[34mraise\u001B[39;49;00m \u001B[36mValueError\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mUnsupported input type. Input type can be a string or an non-empty list. \u001B[39;49;00m\u001B[33m\\\u001B[39;49;00m\n",
      "\u001B[33m                             I got \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(data))\u001B[37m\u001B[39;49;00m\n",
      "                       \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m#encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m#encoded = tokenizer(data, add_special_tokens=True) \u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# for backward compatibility use the following way to encode \u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# https://github.com/huggingface/transformers/issues/5580\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        input_ids = [tokenizer.encode(x, add_special_tokens=\u001B[34mTrue\u001B[39;49;00m) \u001B[34mfor\u001B[39;49;00m x \u001B[35min\u001B[39;49;00m data]\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m================ encoded sentences ==============\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(input_ids)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# pad shorter sentence\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        padded =  torch.zeros(\u001B[36mlen\u001B[39;49;00m(input_ids), MAX_LEN) \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mfor\u001B[39;49;00m i, p \u001B[35min\u001B[39;49;00m \u001B[36menumerate\u001B[39;49;00m(input_ids):\u001B[37m\u001B[39;49;00m\n",
      "            padded[i, :\u001B[36mlen\u001B[39;49;00m(p)] = torch.tensor(p)\u001B[37m\u001B[39;49;00m\n",
      "     \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m# create mask\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "        mask = (padded != \u001B[34m0\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m================= padded input and attention mask ================\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(padded, \u001B[33m'\u001B[39;49;00m\u001B[33m\\n\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, mask)\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[34mreturn\u001B[39;49;00m padded.long(), mask.long()\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mraise\u001B[39;49;00m \u001B[36mValueError\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mUnsupported content type: \u001B[39;49;00m\u001B[33m{}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m.format(request_content_type))\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mpredict_fn\u001B[39;49;00m(input_data, model):\u001B[37m\u001B[39;49;00m\n",
      "    device = torch.device(\u001B[33m\"\u001B[39;49;00m\u001B[33mcuda\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[34mif\u001B[39;49;00m torch.cuda.is_available() \u001B[34melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mcpu\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    model.to(device)\u001B[37m\u001B[39;49;00m\n",
      "    model.eval()\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    input_id, input_mask = input_data\u001B[37m\u001B[39;49;00m\n",
      "    input_id = input_id.to(device)\u001B[37m\u001B[39;49;00m\n",
      "    input_mask = input_mask.to(device)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m============== encoded data =================\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[36mprint\u001B[39;49;00m(input_id, input_mask)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mwith\u001B[39;49;00m torch.no_grad():\u001B[37m\u001B[39;49;00m\n",
      "        y = model(input_id, attention_mask=input_mask)[\u001B[34m0\u001B[39;49;00m]\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m=============== inference result =================\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mprint\u001B[39;49;00m(y)\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[34mreturn\u001B[39;49;00m y\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "\u001B[34mif\u001B[39;49;00m \u001B[31m__name__\u001B[39;49;00m == \u001B[33m\"\u001B[39;49;00m\u001B[33m__main__\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m:\u001B[37m\u001B[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# Data and model checkpoints directories\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33m--num_labels\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m2\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mN\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33minput batch size for training (default: 64)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33m--batch-size\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m64\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mN\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33minput batch size for training (default: 64)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33m--test-batch-size\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m1000\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mN\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33minput batch size for testing (default: 1000)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--epochs\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m2\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mN\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33mnumber of epochs to train (default: 10)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--lr\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mfloat\u001B[39;49;00m, default=\u001B[34m0.01\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mLR\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33mlearning rate (default: 0.01)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--momentum\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mfloat\u001B[39;49;00m, default=\u001B[34m0.5\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mM\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33mSGD momentum (default: 0.5)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--seed\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m1\u001B[39;49;00m, metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mS\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, help=\u001B[33m\"\u001B[39;49;00m\u001B[33mrandom seed (default: 1)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33m--log-interval\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        default=\u001B[34m50\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        metavar=\u001B[33m\"\u001B[39;49;00m\u001B[33mN\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        help=\u001B[33m\"\u001B[39;49;00m\u001B[33mhow many batches to wait before logging training status\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[33m\"\u001B[39;49;00m\u001B[33m--backend\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        default=\u001B[34mNone\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "        help=\u001B[33m\"\u001B[39;49;00m\u001B[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[37m\u001B[39;49;00m\n",
      "    )\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    \u001B[37m# Container environment\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--hosts\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mlist\u001B[39;49;00m, default=json.loads(os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mSM_HOSTS\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]))\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--current-host\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mSM_CURRENT_HOST\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--model-dir\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mSM_MODEL_DIR\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--data-dir\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mSM_CHANNEL_TRAINING\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--test\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mSM_CHANNEL_TESTING\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\u001B[37m\u001B[39;49;00m\n",
      "    parser.add_argument(\u001B[33m\"\u001B[39;49;00m\u001B[33m--num-gpus\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=os.environ[\u001B[33m\"\u001B[39;49;00m\u001B[33mSM_NUM_GPUS\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m])\u001B[37m\u001B[39;49;00m\n",
      "\u001B[37m\u001B[39;49;00m\n",
      "    train(parser.parse_args())\u001B[37m\u001B[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train_deploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3c53e-6168-43e3-a538-ede17b8d2ce3",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf09883-03fc-4c80-91be-d90dbdb3cae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-08 09:07:59 Starting - Starting the training job...\n",
      "2024-06-08 09:08:17 Starting - Preparing the instances for training...\n",
      "2024-06-08 09:08:52 Downloading - Downloading the training image...\n",
      "2024-06-08 09:09:22 Training - Training image download completed. Training in progress.....\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34m2024-06-08 09:09:52,484 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2024-06-08 09:09:52,485 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:09:52,485 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:09:52,495 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2024-06-08 09:09:52,497 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2024-06-08 09:09:53,997 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.66.4)\u001B[0m\n",
      "\u001B[34mCollecting requests==2.22.0 (from -r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\u001B[0m\n",
      "\u001B[34mCollecting regex (from -r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001B[0m\n",
      "\u001B[34m 40.9/40.9 kB 4.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting sentencepiece (from -r requirements.txt (line 4))\u001B[0m\n",
      "\u001B[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001B[0m\n",
      "\u001B[34mCollecting sacremoses (from -r requirements.txt (line 5))\u001B[0m\n",
      "\u001B[34mDownloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\u001B[0m\n",
      "\u001B[34mCollecting transformers==4.41.0 (from -r requirements.txt (line 6))\u001B[0m\n",
      "\u001B[34mDownloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\u001B[0m\n",
      "\u001B[34m 43.8/43.8 kB 7.3 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mCollecting mlflow==2.13.1 (from -r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading mlflow-2.13.1-py3-none-any.whl.metadata (29 kB)\u001B[0m\n",
      "\u001B[34mCollecting pytorch-lightning (from -r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading pytorch_lightning-2.2.5-py3-none-any.whl.metadata (21 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.1.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.4.2)\u001B[0m\n",
      "\u001B[34mCollecting chardet<3.1.0,>=3.0.2 (from requests==2.22.0->-r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting idna<2.9,>=2.5 (from requests==2.22.0->-r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading idna-2.8-py2.py3-none-any.whl.metadata (8.9 kB)\u001B[0m\n",
      "\u001B[34mCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests==2.22.0->-r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mDownloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\u001B[0m\n",
      "\u001B[34m 41.1/41.1 kB 6.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2024.2.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0->-r requirements.txt (line 6)) (3.13.1)\u001B[0m\n",
      "\u001B[34mCollecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.0->-r requirements.txt (line 6))\u001B[0m\n",
      "\u001B[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0->-r requirements.txt (line 6)) (1.26.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0->-r requirements.txt (line 6)) (23.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.0->-r requirements.txt (line 6)) (6.0.1)\u001B[0m\n",
      "\u001B[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.41.0->-r requirements.txt (line 6))\u001B[0m\n",
      "\u001B[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001B[0m\n",
      "\u001B[34mCollecting safetensors>=0.4.1 (from transformers==4.41.0->-r requirements.txt (line 6))\u001B[0m\n",
      "\u001B[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001B[0m\n",
      "\u001B[34mCollecting Flask<4 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting alembic!=1.10.0,<2 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\u001B[0m\n",
      "\u001B[34mCollecting cachetools<6,>=5.0.0 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (8.1.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (2.2.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (7.0.0)\u001B[0m\n",
      "\u001B[34mCollecting entrypoints<1 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\u001B[0m\n",
      "\u001B[34mCollecting gitpython<4,>=3.1.9 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\u001B[0m\n",
      "\u001B[34mCollecting graphene<4 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (6.11.0)\u001B[0m\n",
      "\u001B[34mCollecting markdown<4,>=3.3 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (3.8.3)\u001B[0m\n",
      "\u001B[34mCollecting opentelemetry-api<3,>=1.0.0 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\u001B[0m\n",
      "\u001B[34mCollecting opentelemetry-sdk<3,>=1.0.0 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (2.2.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (3.20.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (15.0.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (2024.1)\u001B[0m\n",
      "\u001B[34mCollecting querystring-parser<2 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (1.4.1.post1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (1.12.0)\u001B[0m\n",
      "\u001B[34mCollecting sqlalchemy<3,>=1.4.0 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\u001B[0m\n",
      "\u001B[34mCollecting sqlparse<1,>=0.4.0 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading sqlparse-0.5.0-py3-none-any.whl.metadata (3.9 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.1->-r requirements.txt (line 7)) (3.1.4)\u001B[0m\n",
      "\u001B[34mCollecting gunicorn<23 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.3.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8)) (2024.2.0)\u001B[0m\n",
      "\u001B[34mCollecting torchmetrics>=0.7.0 (from pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->-r requirements.txt (line 8)) (4.10.0)\u001B[0m\n",
      "\u001B[34mCollecting lightning-utilities>=0.8.0 (from pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (1.12)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (3.2.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.10/site-packages (from s3fs->-r requirements.txt (line 10)) (1.34.52)\u001B[0m\n",
      "\u001B[34mCollecting Mako (from alembic!=1.10.0,<2->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs->-r requirements.txt (line 10)) (1.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs->-r requirements.txt (line 10)) (2.8.2)\u001B[0m\n",
      "\u001B[34mINFO: pip is looking at multiple versions of docker to determine which version is compatible with other requirements. This could take a while.\u001B[0m\n",
      "\u001B[34mCollecting docker<8,>=4.0.0 (from mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\u001B[0m\n",
      "\u001B[34mDownloading docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\u001B[0m\n",
      "\u001B[34mDownloading docker-6.1.2-py3-none-any.whl.metadata (3.5 kB)\u001B[0m\n",
      "\u001B[34mDownloading docker-6.1.1-py3-none-any.whl.metadata (3.5 kB)\u001B[0m\n",
      "\u001B[34mDownloading docker-6.1.0-py3-none-any.whl.metadata (3.5 kB)\u001B[0m\n",
      "\u001B[34mDownloading docker-6.0.1-py3-none-any.whl.metadata (3.4 kB)\u001B[0m\n",
      "\u001B[34mDownloading docker-6.0.0-py3-none-any.whl.metadata (3.4 kB)\u001B[0m\n",
      "\u001B[34mINFO: pip is still looking at multiple versions of docker to determine which version is compatible with other requirements. This could take a while.\u001B[0m\n",
      "\u001B[34mDownloading docker-5.0.3-py2.py3-none-any.whl.metadata (3.5 kB)\u001B[0m\n",
      "\u001B[34mCollecting websocket-client>=0.32.0 (from docker<8,>=4.0.0->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.13.1->-r requirements.txt (line 7)) (3.0.3)\u001B[0m\n",
      "\u001B[34mCollecting itsdangerous>=2.1.2 (from Flask<4->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\u001B[0m\n",
      "\u001B[34mCollecting blinker>=1.6.2 (from Flask<4->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\u001B[0m\n",
      "\u001B[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001B[0m\n",
      "\u001B[34mCollecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\u001B[0m\n",
      "\u001B[34mCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\u001B[0m\n",
      "\u001B[34mCollecting aniso8601<10,>=8 (from graphene<4->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.13.1->-r requirements.txt (line 7)) (3.17.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.13.1->-r requirements.txt (line 7)) (2.1.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning->-r requirements.txt (line 8)) (68.1.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.1->-r requirements.txt (line 7)) (1.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.1->-r requirements.txt (line 7)) (0.12.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.1->-r requirements.txt (line 7)) (4.49.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.1->-r requirements.txt (line 7)) (1.4.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.1->-r requirements.txt (line 7)) (10.3.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.1->-r requirements.txt (line 7)) (3.1.1)\u001B[0m\n",
      "\u001B[34mCollecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.0.0->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\u001B[0m\n",
      "\u001B[34mCollecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-sdk<3,>=1.0.0->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow==2.13.1->-r requirements.txt (line 7)) (2024.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow==2.13.1->-r requirements.txt (line 7)) (1.16.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.13.1->-r requirements.txt (line 7)) (3.3.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.13.1->-r requirements.txt (line 7)) (3.0.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 9)) (1.3.0)\u001B[0m\n",
      "\u001B[34mCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8)) (23.2.0)\u001B[0m\n",
      "\u001B[34mCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001B[0m\n",
      "\u001B[34mCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001B[0m\n",
      "\u001B[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001B[0m\n",
      "\u001B[34mCollecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001B[0m\n",
      "\u001B[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow==2.13.1->-r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001B[0m\n",
      "\u001B[34mDownloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001B[0m\n",
      "\u001B[34m 58.0/58.0 kB 11.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\u001B[0m\n",
      "\u001B[34m 9.1/9.1 MB 108.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading mlflow-2.13.1-py3-none-any.whl (25.0 MB)\u001B[0m\n",
      "\u001B[34m 25.0/25.0 MB 75.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\u001B[0m\n",
      "\u001B[34m 775.1/775.1 kB 67.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001B[0m\n",
      "\u001B[34m 1.3/1.3 MB 83.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\u001B[0m\n",
      "\u001B[34m 897.5/897.5 kB 59.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\u001B[0m\n",
      "\u001B[34m 802.3/802.3 kB 66.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\u001B[0m\n",
      "\u001B[34m 233.4/233.4 kB 39.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\u001B[0m\n",
      "\u001B[34mDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\u001B[0m\n",
      "\u001B[34m 133.4/133.4 kB 25.7 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\u001B[0m\n",
      "\u001B[34m 146.2/146.2 kB 27.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001B[0m\n",
      "\u001B[34mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001B[0m\n",
      "\u001B[34m 101.7/101.7 kB 19.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001B[0m\n",
      "\u001B[34m 207.3/207.3 kB 29.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\u001B[0m\n",
      "\u001B[34m 128.2/128.2 kB 22.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\u001B[0m\n",
      "\u001B[34m 84.4/84.4 kB 16.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001B[0m\n",
      "\u001B[34m 401.7/401.7 kB 49.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001B[0m\n",
      "\u001B[34m 58.6/58.6 kB 9.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\u001B[0m\n",
      "\u001B[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001B[0m\n",
      "\u001B[34m 105.4/105.4 kB 19.5 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\u001B[0m\n",
      "\u001B[34m 59.9/59.9 kB 9.5 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\u001B[0m\n",
      "\u001B[34m 107.0/107.0 kB 21.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\u001B[0m\n",
      "\u001B[34m 130.5/130.5 kB 22.9 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001B[0m\n",
      "\u001B[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001B[0m\n",
      "\u001B[34m 1.2/1.2 MB 76.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001B[0m\n",
      "\u001B[34m 3.1/3.1 MB 108.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading sqlparse-0.5.0-py3-none-any.whl (43 kB)\u001B[0m\n",
      "\u001B[34m 44.0/44.0 kB 7.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001B[0m\n",
      "\u001B[34m 3.6/3.6 MB 106.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\u001B[0m\n",
      "\u001B[34m 868.8/868.8 kB 64.8 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\u001B[0m\n",
      "\u001B[34m 128.0/128.0 kB 21.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001B[0m\n",
      "\u001B[34m 1.2/1.2 MB 82.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\u001B[0m\n",
      "\u001B[34m 52.8/52.8 kB 9.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001B[0m\n",
      "\u001B[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001B[0m\n",
      "\u001B[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001B[0m\n",
      "\u001B[34m 62.7/62.7 kB 11.1 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\u001B[0m\n",
      "\u001B[34m 202.9/202.9 kB 31.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001B[0m\n",
      "\u001B[34mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001B[0m\n",
      "\u001B[34mDownloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\u001B[0m\n",
      "\u001B[34m 58.8/58.8 kB 10.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\u001B[0m\n",
      "\u001B[34m 78.6/78.6 kB 13.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001B[0m\n",
      "\u001B[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001B[0m\n",
      "\u001B[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001B[0m\n",
      "\u001B[34m 239.5/239.5 kB 39.4 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001B[0m\n",
      "\u001B[34m 124.3/124.3 kB 22.0 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001B[0m\n",
      "\u001B[34mDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\u001B[0m\n",
      "\u001B[34m 80.3/80.3 kB 14.6 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001B[0m\n",
      "\u001B[34m 301.6/301.6 kB 45.2 MB/s eta 0:00:00\u001B[0m\n",
      "\u001B[34mInstalling collected packages: sentencepiece, chardet, aniso8601, wrapt, websocket-client, urllib3, sqlparse, sqlalchemy, smmap, safetensors, regex, querystring-parser, multidict, markdown, Mako, lightning-utilities, itsdangerous, idna, gunicorn, graphql-core, frozenlist, entrypoints, cachetools, blinker, async-timeout, yarl, sacremoses, requests, graphql-relay, gitdb, Flask, deprecated, alembic, aiosignal, torchmetrics, opentelemetry-api, huggingface-hub, graphene, gitpython, docker, aiohttp, tokenizers, opentelemetry-semantic-conventions, transformers, pytorch-lightning, opentelemetry-sdk, mlflow\u001B[0m\n",
      "\u001B[34mAttempting uninstall: urllib3\u001B[0m\n",
      "\u001B[34mFound existing installation: urllib3 1.26.18\u001B[0m\n",
      "\u001B[34mUninstalling urllib3-1.26.18:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled urllib3-1.26.18\u001B[0m\n",
      "\u001B[34mAttempting uninstall: idna\u001B[0m\n",
      "\u001B[34mFound existing installation: idna 3.7\u001B[0m\n",
      "\u001B[34mUninstalling idna-3.7:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled idna-3.7\u001B[0m\n",
      "\u001B[34mAttempting uninstall: requests\u001B[0m\n",
      "\u001B[34mFound existing installation: requests 2.32.3\u001B[0m\n",
      "\u001B[34mUninstalling requests-2.32.3:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled requests-2.32.3\u001B[0m\n",
      "\u001B[34mAttempting uninstall: docker\u001B[0m\n",
      "\u001B[34mFound existing installation: docker 7.0.0\u001B[0m\n",
      "\u001B[34mUninstalling docker-7.0.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled docker-7.0.0\u001B[0m\n",
      "\u001B[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001B[0m\n",
      "\u001B[34msagemaker 2.221.1 requires urllib3<3.0.0,>=1.26.8, but you have urllib3 1.25.11 which is incompatible.\u001B[0m\n",
      "\u001B[34mSuccessfully installed Flask-3.0.3 Mako-1.3.5 aiohttp-3.9.5 aiosignal-1.3.1 alembic-1.13.1 aniso8601-9.0.1 async-timeout-4.0.3 blinker-1.8.2 cachetools-5.3.3 chardet-3.0.4 deprecated-1.2.14 docker-5.0.3 entrypoints-0.4 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 huggingface-hub-0.23.3 idna-2.8 itsdangerous-2.2.0 lightning-utilities-0.11.2 markdown-3.6 mlflow-2.13.1 multidict-6.0.5 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 pytorch-lightning-2.2.5 querystring-parser-1.2.4 regex-2024.5.15 requests-2.22.0 sacremoses-0.1.1 safetensors-0.4.3 sentencepiece-0.2.0 smmap-5.0.1 sqlalchemy-2.0.30 sqlparse-0.5.0 tokenizers-0.19.1 torchmetrics-1.4.0.post0 transformers-4.41.0 urllib3-1.25.11 websocket-client-1.8.0 wrapt-1.16.0 yarl-1.9.4\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,339 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,339 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,340 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,341 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,352 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,352 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,364 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,365 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,375 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 5,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2024-06-08-09-07-59-579\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://model-storage-05062024/pytorch-training-2024-06-08-09-07-59-579/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":5,\"num_labels\":2}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=train_deploy.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"testing\",\"training\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.4xlarge\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}}\u001B[0m\n",
      "\u001B[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001B[0m\n",
      "\u001B[34mSM_IS_HETERO=false\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=train_deploy\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=16\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=0\u001B[0m\n",
      "\u001B[34mSM_NUM_NEURONS=0\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://model-storage-05062024/pytorch-training-2024-06-08-09-07-59-579/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":5,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2024-06-08-09-07-59-579\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://model-storage-05062024/pytorch-training-2024-06-08-09-07-59-579/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"5\",\"--num_labels\",\"2\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001B[0m\n",
      "\u001B[34mSM_HP_BACKEND=gloo\u001B[0m\n",
      "\u001B[34mSM_HP_EPOCHS=5\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_LABELS=2\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.10 train_deploy.py --backend gloo --epochs 5 --num_labels 2\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,376 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001B[0m\n",
      "\u001B[34m2024-06-08 09:10:14,377 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001B[0m\n",
      "\u001B[34mLoading BERT tokenizer...\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[34mDistributed training - False\u001B[0m\n",
      "\u001B[34mNumber of gpus available - 0\u001B[0m\n",
      "\u001B[34mGet train data loader\u001B[0m\n",
      "\u001B[34mProcesses 75/75 (100%) of train data\u001B[0m\n",
      "\u001B[34mProcesses 25/25 (100%) of test data\u001B[0m\n",
      "\u001B[34mStarting BertForSequenceClassification\u001B[0m\n",
      "\u001B[34mDEBUG:__main__:Processes 75/75 (100%) of train data\u001B[0m\n",
      "\u001B[34mDEBUG:__main__:Processes 25/25 (100%) of test data\u001B[0m\n",
      "\u001B[34mINFO:__main__:Starting BertForSequenceClassification\u001B[0m\n",
      "\u001B[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001B[0m\n",
      "\u001B[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n",
      "\u001B[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001B[0m\n",
      "\u001B[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[34mEnd of defining BertForSequenceClassification\u001B[0m\n",
      "\u001B[34mINFO:__main__:End of defining BertForSequenceClassification\u001B[0m\n",
      "\u001B[34mTrain Epoch: 1 [0/75 (0%)] Loss: 0.678380\u001B[0m\n",
      "\u001B[34mINFO:__main__:Train Epoch: 1 [0/75 (0%)] Loss: 0.678380\u001B[0m\n",
      "\u001B[34mAverage training loss: 0.645679\u001B[0m\n",
      "\u001B[34mINFO:__main__:Average training loss: 0.645679\u001B[0m\n",
      "\u001B[34mtrue_vals\u001B[0m\n",
      "\u001B[34m[1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1]\u001B[0m\n",
      "\u001B[34mtrue_vals len 25\u001B[0m\n",
      "\u001B[34mpreds_flat\u001B[0m\n",
      "\u001B[34m[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\u001B[0m\n",
      "\u001B[34mpreds_flat len 25\u001B[0m\n",
      "\u001B[34mClass: negative\u001B[0m\n",
      "\u001B[34mAccuracy:12/12\u001B[0m\n",
      "\u001B[34mClass: positive\u001B[0m\n",
      "\u001B[34mAccuracy:0/13\u001B[0m\n",
      "\u001B[34mTest set: Accuracy: 0.480000\u001B[0m\n",
      "\u001B[34mINFO:__main__:Test set: Accuracy: 0.480000\u001B[0m\n",
      "\u001B[34mTrain Epoch: 2 [0/75 (0%)] Loss: 0.647546\u001B[0m\n",
      "\u001B[34mINFO:__main__:Train Epoch: 2 [0/75 (0%)] Loss: 0.647546\u001B[0m\n",
      "\u001B[34mAverage training loss: 0.660126\u001B[0m\n",
      "\u001B[34mINFO:__main__:Average training loss: 0.660126\u001B[0m\n",
      "\u001B[34mtrue_vals\u001B[0m\n",
      "\u001B[34m[1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0]\u001B[0m\n",
      "\u001B[34mtrue_vals len 25\u001B[0m\n",
      "\u001B[34mpreds_flat\u001B[0m\n",
      "\u001B[34m[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\u001B[0m\n",
      "\u001B[34mpreds_flat len 25\u001B[0m\n",
      "\u001B[34mClass: negative\u001B[0m\n",
      "\u001B[34mAccuracy:12/12\u001B[0m\n",
      "\u001B[34mClass: positive\u001B[0m\n",
      "\u001B[34mAccuracy:0/13\u001B[0m\n",
      "\u001B[34mTest set: Accuracy: 0.480000\u001B[0m\n",
      "\u001B[34mINFO:__main__:Test set: Accuracy: 0.480000\u001B[0m\n",
      "\u001B[34mTrain Epoch: 3 [0/75 (0%)] Loss: 0.603920\u001B[0m\n",
      "\u001B[34mINFO:__main__:Train Epoch: 3 [0/75 (0%)] Loss: 0.603920\u001B[0m\n",
      "\u001B[34mAverage training loss: 0.629522\u001B[0m\n",
      "\u001B[34mINFO:__main__:Average training loss: 0.629522\u001B[0m\n",
      "\u001B[34mtrue_vals\u001B[0m\n",
      "\u001B[34m[0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0]\u001B[0m\n",
      "\u001B[34mtrue_vals len 25\u001B[0m\n",
      "\u001B[34mpreds_flat\u001B[0m\n",
      "\u001B[34m[0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\u001B[0m\n",
      "\u001B[34mpreds_flat len 25\u001B[0m\n",
      "\u001B[34mClass: negative\u001B[0m\n",
      "\u001B[34mAccuracy:12/12\u001B[0m\n",
      "\u001B[34mClass: positive\u001B[0m\n",
      "\u001B[34mAccuracy:4/13\u001B[0m\n",
      "\u001B[34mTest set: Accuracy: 0.640000\u001B[0m\n",
      "\u001B[34mINFO:__main__:Test set: Accuracy: 0.640000\u001B[0m\n",
      "\u001B[34mTrain Epoch: 4 [0/75 (0%)] Loss: 0.572580\u001B[0m\n",
      "\u001B[34mINFO:__main__:Train Epoch: 4 [0/75 (0%)] Loss: 0.572580\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# place to save model artifact\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,  # this script only support distributed training for GPU instances.\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 5,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    },\n",
    "    disable_profiler=True,\n",
    ")\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd22efb-5654-47a8-897e-12007675972e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
